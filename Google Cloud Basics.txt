Google Cloud Basics

Azz92z23P2hvtRyy - 50% Cert Voucher

Cloud SDK and CLI
---------------------------------    
gcloud (compute engine, iam, etc)
  gcloud <component> <entity> <operation> <args> <flags> // syntax
	gcloud init
	gcloud auth login
	glcoud config list
	gcloud compute ssh instance-1 --zone=us-east1-b
	gcloud compute firewall-rules create "prod-ssh-prod" --network low-sandbox-prod-vpc --source-ranges 0.0.0.0/0 --allow tcp:22
	
	
gsutil (cloud storage)

bq (big query)

kubectl (k8)



Geographical Concepts
-----------------------------
Regions (us-east1)
 	Zones (i.e. AWS Availability Zones; us-east1-b)
  	Edge Locations (i.e AWS Cloud Front, Azure Front Door)
     
     

Account Structure
-----------------------
Organization
	Folder (Departments or Teams)
		Project (i.e. Azure Subscription)
			Resources (all resources are bound to a project)
     
**A project includes a set of users, APIs, billing, authentication, and monitoring



Resource Manager
-------------------------

[ Cloud Asset Inventory ]

[ Organization Policy Service ]




Billing
---------

Payments Profile -> Billing Account -> Project 1 + Project 2 ...

[ Methods ]

Self-Service
	online billing linked to a debit/credit card
Invoiced
	handled via wire transfers and mail (checks)


Service APIs
------------------
- allow you to interact with GCP services programmatically
- through the API Management Console you can monitor the requests, traffic, error and latency on any ENABLED API
- can create API credentials
- have access to the API library

gcloud services enable // enables service APIs from the command line



IAM
--------
Who (Identity) -> Has What Access (Role) -> For Which Resource

Member/Principal
	Role
		Condition
			Metadata
				Audit Config

[ Member ] 
- email address associated with a user, service account, or google group

[ Role ]  
- collection of permissions that indicate what a member can do on a resource
- permissions are not granted to users directly, but to roles
- permissions correspond 1-1 with REST API methods
- multiple roles can be granted to one user

[ Types of Roles ]

Basic
	historical roles that existed in GCP
	include Owner, Editor, Viewer
	avoid these as they are too broad
	
Predefined (Preferred)
	finer grained access-control 
	created and maintained by Google
	
Custom
	user defined 
	
[ Condition ]
- grant resource access to identities only if configured conditions are met


[ Policy ]
- 1 policy per resource
- 100 conditional bindings per policy
- 1500 members or 250 Google groups per policy
- takes up to 7 minutes for policies to update globally


[ Service Accounts ]
- associated with a public key pair (user or googled-managed)
- can be attached to resources (i.e. AWS Instance Role)
- can be impersonated by people (i.e. Assuming a role)

User-Managed
	you create the name
	<service-account-name>@<project-id>.iam.gserviceaccount.com
	
Default
	<project-id>@appspot.gserviceaccount.com
	<project-number>-compute@developer.gserviceaccount.com


Google-Managed
	


[ Cloud Identity Domain (i.e. Active Directory)]
- google accounts that are in an organization, but ARE NOT tied to any GSuite applications or features

AllAuthentitcated Users
	all service accounts or users on the internet who have authenticated with a Google account
	
AllUsers
	anyone who is on the internet, including authenticated and unauthenticated users
	
	
[ Cloud Identity Platform ]
- enterprise-level access and identity management platform (think AWS Cognito, Azure Entra ID)
- supports OpenID Connect and SAML
- supports integration with IAP (below)

[ Cloud Identity Aware Proxy - IAP ]
- managed solution for authenticating and authorizing users
- establishes a central authentication layer for apps accessed via HTTP/S (think Keycloak, Auth0)
- supports Compute Engine, App Engine, and GKE

1. Users are directed to Google to authenticate to get an Access Token
2. Authorization is driven via IAM



Networking
---------------

VPC
	Subnets
	 	Routes
	 		Firewall Rules
	

[ VPC ]
- have a global presence
- no concept of an "Internet Gateway" (like Azure)
- instances will have internet access IF they have an external IP associated with them

// VPC Service Controls
- defines a service perimeter around protected resources
- mitigates the risk of data exfiltration from services like Cloud Storage and BigQuery
- enforces special rules at the border
- is independent of IAM or firewall policies (higher level)

Inbound Access FROM authorized VPC networks only
Outbound Access TO whitelisted IP ranges only

[ Private Service Connect ]
- allows consumers (me) to access managed services (e.g. Cloud SQL, BigQuery) privately from inside my VPC
- I can use my own internal IP addresses to access services without leaving my VPC
- traffic is sent to endpoints and backends (think AWS endpoints) and then routed to the target service

[ Private Service Access ]
- similar to PSC, but requires me to allocate a range of IP address to create a connection
- 
[ Shared VPC ]
- connects resources from multiple projects to a common network
- resources can communicate with each other securely and efficiently using internal IPs from that network
- you must designate a project as a "host" project and attach one or more other "service" projects to it
- shared VPC connects projects within the same organization

Service Project -> Host Project <- Service Project

// Constraints

A project that participates in Shared VPC is either a host project or a service project:

A host project contains one or more Shared VPC networks. A Shared VPC Admin must first enable a project as a host project. After that, a Shared VPC Admin can attach one or more service projects to it.

A service project is any project that has been attached to a host project by a Shared VPC Admin. This attachment allows it to participate in Shared VPC. It's a common practice to have multiple service projects operated and administered by different departments or teams in your organization.

A project cannot be both a host project and a service project simultaneously. Thus, a service project cannot be a host project to further service projects.

You can create and use multiple host projects; however, each service project can only be attached to a single host project. For an illustration, see the multiple host projects example.

You can create networks, subnets, secondary address ranges, firewall rules, and other network resources in the host project. The host project can then share selected subnets, including secondary ranges, with the service projects. Services running in a service project can use Shared VPC to communicate with resources running in the other service projects.
	
	
[ Subnets ]
- subnets are associated with a REGION; instances are associated with a ZONE
- by default instances MUST have a public IP to access Google Cloud APIs (cloud storage, etc)
- this can be done internally by enabling "Private Google Access" on the subnet
**subnets can be EXTENDED without stoping resources or deleting the current subnet -- UNIQUE

Reserved IP Address Per Subnet
	1st address - network
	2nd address - gateway
	2nd-to-last address - future use
	Last address - broadcast
	

[ External IP Addresses ]
- reserve static IP addresses and assign them to resources
- there is no charge if they are assigned and are in-use
- there is a charge if they are reserved and NOT used

// Types
Regional - can be assigned to Compute Engine and Network Loadbalancers
Global - can be assigned to anycast IPs and global loadbalancers


[ Routes ] 

Default Route
	path to the internet
	path to Private Google Access (APIs)
	
Subnet Route
	defines path to each subnet in the VPC
	
Custom Routes
	static - created for VPN tunnels
	dynamic - for cloud routers and dynamically routed VPN appliances

[ Firewall Rules ]
 - used to determine what flows into, out of, and within the VPC
 - the lower the priority number the higher the importance
 - network tags serve as "security groups" which can be applied to instances in order to apply certain rules directly to groups of instances
 
 **Firewall Rules can be configured at the organizational or folder level (Hierarchical Firewall Policy); they will be inherited by the projects below them
 
 // 7 Components
 Direction -- Ingress/Egress
 Priority -- High/Low
 Action -- Allow/Deny
 Status -- Enabled/Disabled
 Target -- Network Tags
 Source -- CIDR Range
 Port/Protocol -- TCP:80,443

[ VPC Peering ]
- no overlapping CIDRs
- non-transitive
- must create ingress firewall rules on both ends 
- network tags do not traverse VPCs


[ Cloud Load-balancing ]
- can be global, regional, external, internal
- are supported by backend services 

HTTPS
	only L7 option
	distributes traffic based on content or geo location
	url or content based (forwarding rules)

SSL Proxy
	L4
	SSL connections are terminated and proxied to backend services
	*support for TCP w/ SSL offload
	
TCP Proxy
	L4
	client TCP connections are terminated at the LB 
	used for non-http traffic 
	forward traffic as SSL or TCP
	
Network
	L4
	not a proxy
	
Internal
	L4
	accessible within the VPC
	

[ Flow Logs ]


[ Cloud Router ]
- provides dynamic routes for hybrid networks 
- links VPCs to external networks
- works with Cloud VPN, and Cloud Interconnect
- automatically learns subnets and announces them to on-premise networks


[ Cloud VPN ]
- must configure a "Peer VPN Gateway" and "Cloud VPN Gateway"
**Cloud VPN should be set-up in it's own account for ease of management

// Options
HA VPN - two interfaces and two external IPs
Classic VPN - single interface and single external IP


[ Cloud Interconnect ]
- think Direct Connect and ExpressRoute
- direct connection into Googles backbone

[ Cross-Cloud Interconnect ]
- establishes dedicated connectivity between Google and another cloud service provider
- 10 / 100 Gbps

// Options
Dedicated - direct connection to GCP; 10-Gbps or 100-Gbps circuits
Partner - connection to a GCP partner


[ Cloud DNS ]
- think AWS Route 53


[ Cloud Domains ]
- global registrar for domain names using Google Domains
- uses built-in DNS
- supports custom nameservers
- supports DNSSEC and private Whols records
- managed as a GCP Project 


Compute
-------------

Compute Engine - used for virtual machine workloads (i.e. AWS EC2, Azure VMs)
	
[ Classes ] 
General Purpose - E2, N2, N2D, N1
Memory Optimized - M1, M2
Compute Optimized - C2
Accelerator Optimized - A2

**You can also create custom machine types that fit your exact specification -- UNIQUE

[ Managed Instance Groups ]
- think AWS Auto-Scaling Groups and Azure VM Scale Sets
- instances that are managed as a single entity and have identical configurations
- can be scaled out and scaled in based on a policy (i.e. CPU)
- deployed across multiple zones

Instance Template -> Instance Group

[ Unmanaged Instance Groups ]
- useful for grouping together VMs that require individual configuration settings or tuning

[ Startup Scripts ]
- think userData from AWS
- can be set at launch time or retrieved from a Cloud Storage Bucket (via a object url)

[ Metadata Service ]
- endpoint available within your environment that allows you to access specific information about your instance on the fly
- http://metadata/computeMetadata/v1/instance/attributes/<node>
- can be access by your startup scripts 

[ Access Control ]
- you can create custom SSH keys and "upload" them to the instance details (metadata)
- you can let Google create/manage the SSH keys that you use to connect to your instances

[ Snapshots ]
- incremental by default
- stored in Cloud Storage (regional or multi-region)
- can be used to create a disk in any region/zone regardless of snapshot source location -- UNIQUE 

[ Monitoring & Logging ]
- install the cloud monitoring agent (based on Fluentd)
- streams data directly to Cloud Logging service
- can also be routed to cloud storage for archiving


Storage
-----------

[ Cloud Storage ]
- used for object storage (i.e. S3)
- buckets are the foundational construct
-	can encrypt data with CMKs or Google Managed Keys
- object lifecycle management for tiering
- retention policy to mandate deletion based on age
- policy locks bind the policy to the bucket; cannot be deleted until all objects are deleted
- object holds to prevent deletion of an object in case of an investigation

// Availability Options
Regional
	Dual-Regional 
  	Multi-Regional
  
// Storage Classes
Standard - instant access
Nearline - 30 day minimum
Coldine - 90 day minimum
Archive - 1 year minimum

**Different classes represent cost efficiency and disaster recovery options

// Transfer/Synchronization Options
Storage Transfer Service - moving data from 3rd party provider to Google; bucket to bucket; more than 1TB from on-prem
Transfer Appliance (Online Mode) - for strong internet connectivity 
Transfer Appliance (Offline Mode) - for poor internet connectivity; transfer data to appliance and ship to google
gsutil - less than 1TB from on-prem


[ Cloud Filestore ] 
- used for file storage (i.e. EFS)
- fully managed NFS service
	

[ Persistent Disk - VM Block Storage ]
- network storage disks that back compute instances (i.e. EBS)
- performance is based on the disk capacity attached to an instance and the number of vCPUs that the instance has
- encrypted by default

// Performance Options
Standard (HDD) 
Balanced (General Purpose) 
SSD

// Disaster Recovery Options
Zonal Persistent Disk - default option (1 zone; 1 region)
Regional Persistent Disk - replicates between 2 zones
Local SSD - very high IOPs and low latency (physically attached; highest performance)
	
	
	
SQL Databases
----------------------

[ Cloud SQL ] 
- highly available 
- supports MySQL, Postgres, and SQL Server engines
- supports Cloud SQL Auth Proxy to enable connections from client applications w/o need for public IP addressing
- database instances are run inside of a Google Managed VPC
- to connect I must configure Private Service Access (for private IP) or use the public IP

// Cloud SQL Auth Proxy
- provides secure access to Cloud SQL instances without the need for authorized networks or for configuring SSL
- installed as a binary based on your OS and kernel architecture
- it runs locally on my machine and I connect using a database client on localhost w/ database credentials

  
 [ Alloy DB ]
 - proprietary Postgres implementation (think AWS Aurora) 
 - high performance, distributed relational database service
 - 4x faster than standard Postgres for transactional workloads
 - compute and storage are separated for efficient scaling
  
[ Cloud Spanner ] 
- enterprise grade
- globally distributed
- strongly consistent cloud first db service
	

No-SQL Databases
---------------------------

[ BigTable ] 
- high performance wide-column NoSQL datastore
- large scale (massive workloads, high throughput, PB scale)
- can scale to billions of rows and thousands of columns
- low-latency
- config changes are immediate; no downtime for reconfiguration
	
// Row Key Design
You can collapse multiple column values into the "Row Key" as ONE value

Vehicle		LAT	LONG	COMPANY	  ROUTE
M117		40.7.   -73.9	       Metro			     86

Vehicle					LAT		LONG
Metro#86#M117 	40.7.  	-73.9	
**Result after performing "Field Promotion" on the Company and Route columns to create a better indexable row key


[ Firestore ] 
- NoSQL datastore
- document database (think MongoDB, AWS DynamoDB)
- rich mobile, web, and IOT applications
- strongly consistent
	
	
[ Firebase ] 
- stores and syncs data between users in real time for mobile sign-ins, applications, ads, and chat
	
	
[ Memory store ] 
- fully managed Redis and Memcached databases
- milisecond data access
	
	
Data Security
------------------

[ Cloud Data Loss Prevention API ] 
- can de-identify sensitive data in text content
- uses a de-identification transformation to mask, delete, or otherwise obscure the data

[ Dataplex ]
- a data fabric that unifies distributed data and automates data management and governance for that data
- organize and catalog data from across the entire organization (Cloud Storage, BiqQuery, etc) without moving that data
	
	
Container Platforms
--------------------------

[ Cloud Run ] 
- think Azure Container Apps/ACI
- platform that allows me to develop and deploy highly scalable containerized applications using my favorite language (Go, Python, Java, Node.js, .NET, and more) in a serverless manner
- can deploy your applications as services or jobs
- can scale to 0 and scale up based on request load -- UNIQUE

Services
	used to run code that responds to web requests, or events
	can be controlled via versions for rollback, traffic splitting between revisions (max 1000 revisions per service)
	can use Ingress settings to control whether the service is public or private
	can scale to 0 and scale up based on request load -- UNIQUE
	can set a minimum of (1) instances if absolutely needed

gcloud run deploy SERVICE --image IMAGE_URL
gcloud run services update-traffic SERVICE --to-latest // send all traffic to latest revision
gcloud run services update-traffic SERVICE --to-revisions REVISION=PERCENTAGE || LIST > hello2-00005-red=25,hello2-00001-bod=25,hello2-00002-nan=50 // multiple
 
Jobs
	used to run code that performs work (a job) and quits when the work is done.
	by default jobs run for 10 minutes; can run up to 7 days max
	
 gcloud run jobs create JOB_NAME --image IMAGE_URL OPTIONS
 gcloud run jobs deploy JOB_NAME --image IMAGE_URL OPTIONS
 gcloud run jobs update JOB_NAME
 gcloud run jobs execute JOB_NAME  --args ARGS --update-env-vars KEY=VALUE>,KEY_N=VALUE_N --tasks TASKS --task-timeout TIMEOUT
 gcloud scheduler jobs create http SCHEDULER_JOB_NAME --location SCHEDULER_REGION --schedule="SCHEDULE" \
  --uri="https://run.googleapis.com/v2/projects/PROJECT-ID/locations/CLOUD_RUN_REGION/jobs/JOB-NAME:run" \
  --http-method POST \
  --oauth-service-account-email PROJECT-NUMBER-compute@developer.gserviceaccount.com


[ Google Kubernetes Engince (GKE) ]
- managed Kubernetes service
- clusters can be private, zonal, or regional 

// Modes
Standard -> pay per node (i.e EKS)
Autopilot -> pay per pod (i.e Fargate)

gcloud container clusters create CLUSTER_NAME --location CONTROL_PLANE_LOCATION --node-locations COMPUTE_ZONE,COMPUTE_ZONE1
gcloud container clusters create-auto CLUSTER_NAME --location=LOCATION --project=PROJECT_ID
gcloud container clusters get-credentials CLUSTER_NAME --location=CONTROL_PLANE_LOCATION
	
// GKE Workload Identity
- maps a Kubernetes service account -> IAM account
- roles can then be assigned to the IAM account so that pods can access other Google Services
	
	
[ Artifact Registry ]
- think AWS Elastic Container Repository (ECR)
- platform used to host container repositories similar to Docker Hub
- used by peer services like Cloud Run and App Engine in order to deploy containers

  gcloud artifacts repositories create REPOSITORY --repository-format=docker --location=LOCATION --description="DESCRIPTION" --async @--allow-vulnerability-scanning
	
	
	
Severless
--------------

[ App Engine ]  
- think Azure App Service
- serverless platform that runs your workloads as code or containers so there are no servers to manage
- supports multiple versions of your application via traffic splitting (percentage distribution)
- integrates with other services like load balancer and cloud sql 
- can scale to 0 and scale up based on request load (Standard) -- UNIQUE

// Modes

Standard
	apps run in Sandbox environment
	specific runtime versions used
	intended to run for free or at a very low cost
	can scale to 0 instances when there is no traffic
	cost based on instance hours

Flexible
	apps run on docker containers
	any versions of runtimes can be used
	cost based on VM resources

// Concept

Application
	Service
		Version
			Instances
	


[ Cloud Run Functions ] 
- think AWS Lambda and Azure Functions
- only ONE trigger can be bound to a function at a time
- source code is deployed to a cloud storage bucket as a zipfile -> Cloud Build prepares a container image -> Pushes it to Container Registry -> Cloud Functions uses this image to run your code

// Types
HTTP Functions - triggered by HTTP requests; there is no API gateway required -- UNIQUE
Background Function -	triggered by events (Pub/Sub, changes in a bucket)


[ Cloud Build ] 
- think AWS CodeBuild
- great for building containers
- include cloudbuild.yml file in the repository


[ Cloud Source Repository ]
- think AWS CodeCommit


[ Cloud Code ]
- plugin that I can install on my favorite IDE (e.g. VSCode)
- integrates w/ Cloud Secrets for secrets management


Asynchronous Messaging
----------------------------------

[ Cloud Task ]
- lets you separate out pieces of work that can be performed independently, outside of your main application flow
- task are placed on a queue to be processed, asynchronously, using handlers that you create
- supported handlers/workers are App Engine, Cloud Functions, Cloud Run, GKE, Compute Engine
- max size of task/message	1MB
- task/message retention 30 days


[ Cloud Pub/Sub ]
- think AWS SNS, AWS SQS, Azure Queues/Service Bus
- global service
- asynchronous messaging service
- focused on messaging between microservices
- max size of task/message	10MB
- task/message retention 7 days
- can be used for streaming data solutions
- can handle any type of data (images, JSON, etc)

[ EventArc ]
- think AWS Eventbridge, Azure Service Bus
- fully managed event routing service for GCP events
- emits events from key services like Cloud Storage, Cloud Run, and SaaS Platforms
- advanced filtering based on attributes


API Management
-----------------------

[ Cloud Endpoints ]
- the classic method
- container based solution
- deploys a sidecar proxy container alongside your primary container (think K8 multi-container pod)
- built on top of nginx 
- v2 of the service uses Envoy as the underlying Proxy Container

[ API Gateway ]
- for lightweight API management 
- completely serverless 
- just provide API configuration spec
- supports the following backends: App Engine, Cloud Run, Cloud Functions

[ Apigee ]
- enterprise grade API management
- offers advance proxy integrations, policies, and transformation pipelines
- offers advanced authentication methods such as SAML, JWT, SAML
- built in console UI
- can support hybrid workloads (cloud or on-prem)

// Concept
Environment
  isolated area within an organization where you can create and deploy API proxies
  environments are created during provisioning, and can later add more environments to your organization.
  API Proxies must be deployed to an environment before they can be used
  environments are members of environment groups
  
Instance 
  an instance is a region-specific virtual machine
  each environment must be attached to at least one Apigee instance
  
  
Environment Group
  defines the way requests are routed to individual environments
  define hostnames on environment groups (not on individual environments) to route requests to the environments within a group using those hostname definitions
  

Vertex AI/ML
------------
- pre trained APIs offered by Google
- most features return a response with a confidence score

// Vertex AI Offerings

[ Cloud Vision API ]

Face Detection
Landmark Detection
Logo Detection
Label Detection
Object Detection
Web Entities
Text Detection (OCR Support)
Explicit Content Detection


[ Video Intelligence API ]

Detects Shot Changes
Tracks Objects
Logo Detection
Transcribing
Label Detection
Text Detection
Explicit Content Detection
Face/Person Detection
Streaming/Live Feed
Celebrity Detection

[ AutoML API ]
- fine tune your own custom models based off Googles models

[ Speech to Text API ]

[ Text to Speech API ] 

[ Translation API ]

[ Natural Language API ]


Big Data
-------------

[ Cloud Compose ]
- think Azure Logics Apps, Azure Data Factory
- fully managed Apache Airflow service
- visually manage your pipelines
- focus on authoring, scheduling, and monitoring your python based workflows

[ Dataproc ]
- managed Apache Spark service for processing large datasets
- Hadoop, MapReduce, Pig/Hiv 
- takes under 90 secs to provision 	
- can turn off when not in use -- UNIQUE
	
[ DataFlow ] 
- managed Apache Bean process executor engine
- unified serverless stream and batch processing service
	
[ Dataprep ]
- think AWS Glue
- visually explores, cleans, and prepares data for analysis and ML
- operated by Trifecta in partnership with Google
- automatically detects schemas, datatypes, possible joins, and outliers
- built-in transformation functions (aggregate, join, union)
- works with csv, JSON, or relational data from Cloud Storage, BigQuery, or direct upload

[ Cloud Data Fusion ]
- think AWS SageMaker instances and Notebooks
- fully managed, cloud-native, enterprise data integration service for quickly building and managing data pipelines
- the web interface lets you build scalable data integration solutions
- it lets you connect to various data sources, transform the data, and then transfer it to various destination systems, without having to manage the infrastructure.

[ Batch ]
- lets you schedule, queue, and execute batch processing workloads on Google Cloud resources
- specify parameters for the resources required for your workload, then Batch obtains resources and queues the job for execution
- a job might be a single shell script or a complex, multipart computation
	
[ BigQuery ]
- serverless, multi-regional, SQL columns-store data warehouse
- can stream at 100k rows per second
- can handle terabytes of data in seconds and petabytes in minutes
- built-in AI/ML and Business Intelligence Engine
- Standard SQL
- supports streaming of data from Pub/Sub, Dataflow, and Datastream
- keeps (7) day history of changes to data
		
[ BigLake ] 
- platform that unifies data-warehouses and data-lakes
- can discover/create tables based on data that lives in Amazon S3 or Azure Data Lake --UNIQUE
- powered by BigQuery
- can apply table, row, column level security policies on object store tables
	
[ Datalab ]
- for visualizing, querying, and interacting with your data using programming languages
- backed by Jupyter Notebook
	
[ Looker ]
- think PowerBi
- business intelligence and reports


[ Looker Studio ]



IoT
------

[ Cloud IoT Core ]
- functions as the endpoint that all IoT devices stream their data to
- data is then forwarded on to Cloud Pub/Sub
- Cloud Functions can then be invoked to process data or perform actions against the IoT device

Devices > Cloud IoT Core > Pub/Sub > Cloud Function
	
	
	
Encryption
--------------

[ Cloud KMS ]
- creates, manages, and apply cryptographic keys
- supports symmentrical and asymmetrical algorithms
- responsible for encrypting and decrypting and controls acces to keys

[ Cloud HSM ]
- host encryption keys and performs cryptographic actions
- FIPS 140-2 Level 3 certified devices
- bound to a region
- keys are not exportable, tamper resistant, provides tamper evidence

[ Cloud EKM - External Key Management ] 
- uses keys from SUPPORTED external key management partners outside of GCP
- supports services such as BigQuery, Compute Engine, Cloud Run, Cloud Spanner, Cloud Storage, GKE, Pub/Sub, Secret Manager
- key ring should be created on the partner side

[ CMEK - Customer Managed Encryption Keys ]
- key is in customers control
- gives you control over more aspects of the lifecycle and management of your keys

[ CSEK - Customer Supplied Encryption Keys ]
- 


Secret Management
-----------------------------

[ Cloud Secret Manager ]
- secrets are immutable once saved
- new versions are created instead
- versions must be referenced when calling the API


	
Operations Suite (i.e Stackdriver)
--------------------------------------------
- a suite of tools used for logging, monitoring, and application diagnostics

[ Cloud Monitoring ] 
- requires setting up a Metrics Scope, formally called Workspaces (i.e. Azure Analyics Workspace)
- should be setup as a "host" project to get insight from all other projects in a single pane of glass
- leverages an agent that streams data from Compute Engine to the service
- supports GKE
- alerting policies
- provides insight into System Metrics (e.g. CPU, disk, etc)
- supports User Defined Metrics (e.g. HTTP status codes)

// Agents
collectd - monitoring
fluentd - logging

// Uptime Checks -- UNIQUE
**can perform the following
Expired SSL checks
Pass custom headers
Perform basic auth 
	
[ Cloud Logging ] 
- can accept logs from Google, AWS, on-prem resources
- supports audit logs (console), access transparency logs (what GCP support staff does), agent logs
- Log Viewer shows logs for one project
- can configure "exclusions" to decide WHAT you really want

// Log Router
**sends logs to different "sinks" or endpoints
1. Default Sink - Default Logs Bucket
2. Big Query
3. Pub/Sub
4. Custom Log Bucket

// Log Types
Audit Logs
	Admin Activity
		contain log entries for API calls or other actions that modify the configuration or metadata of resources
		these logs are always written
		cannot configure, exclude, or disable them
		
	Data Access
		contain API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data
	
	System Event
		entries for Google Cloud actions that modify the configuration of resources
		generated by Google systems
		they aren't driven by direct user action
	
	Policy Denied
		recorded when a Google Cloud service denies access to a user or service account because of a security policy violation
		security policies are determined by VPC Service Controls
	
[ Cloud Error Reporting ]  -- UNIQUE
- real time error monitoring and alerting
- consolidates all errors within a single pane of glass
- errors are deduplicated and grouped together into an "issue"
- how many occurrences? when was it last seen?
- integrated into Cloud Functions, and App Engine (standard)

**Makes it incredibly easy to locate, discover, and address errors in apps
	
[ Cloud Debugger ]
- debugs a running application without interrupting the live service
- the service takes a snapshot of the live service 
- allows you to insert breakpoints in the snapshot of the app and look for errors
	
[ Cloud Trace ]
- instrument my apps w/ OpenTelemetry, OpenCensus, Google SDK

Trace - the total time it takes for a request to make it end-to-end
Span - the time it took for a particular "segment" of a request path

[ Cloud Profiler ] 
- continuously analyzes the performance of your application
- focuses on metrics like CPU time, Heap, Contention, Threads, etc
- maintains profiles for 30 days
- agent based; requires instrumentation
- supports Compute Engine, App Engine, GKE, Dataflow, Dataproc 
	
	
	
Cloud Security Command Center -- UNIQUE
-------------------------------------------
- a suite of tools used to help improve/maintain the security posture of a project/organization
- more so designed to be configured for an organization
- designed to prevent, detect, and respond to threats
- provides a "Single Pane of Glass" 
- identifies security compliance violations and misconfigurations in GCP Assets (think AWS Config)

// Tiers
Standard
	Security Health Analytics
	Web Security Scanner
	Cloud Armor
	Cloud Data Loss Prevention (DLP)
	Anomaly Detection
	
Premium
	Security Health Analytics + Compliance Reporting
	Web Security Scanner + Managed Scans
	Event Threat Detection
	Container Threat Detection
	Exports to Pub/Sub

[ Web Security Scanner ]
- will crawl a url of a Compute Engine, App Engine, or GKE web app
- checks for common vulnerabilities and exploits
- create a series of "scans"
- can set authentication method and frequency of the scan 
- work in progess; not highly valuable at the moment


[ Container Scanning API ]
- performs container scanning on images in Artifact Registry


[ Event Threat Detection ]
- identify threats in real-time by monitoring and analyzing Cloud Logging
- threats are defined by "rules" which specify targeted logs 
- support "custom" rules by running queries on logs exported to BigQuery

[ Cloud Armor ]
- edge level DDos protection and web application firewall (WAF)
- built on top of Cloud Loadbalancing
- mitigates OWASP top ten risks (Open Web Application Security Project)
- whitelist traffic based on source IP or CIDR Ranges
- preview changes before making policy live




Multi-Cloud + On-Prem
--------------------------------

[ Anthos ]
- allows for application development ANYWHERE in conjunction w/ a Google Cloud Control Plane
- supports GKE, Cloud Run, and Compute Engine

[ Anthos Service Mesh ]
- enables managed, observable, and secure communication across microservices, on-premises, and on GCP
- powered by Istio; implements a control plane and monitors all traffic through a proxy




	


