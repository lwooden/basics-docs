Kubernetes is an open source system for automating deployment, scaling, and management of containerized applications.

At heart it is a "container orchestration" tool

With containers we can run a variety of software components across a "cluster" of generic servers; instead of running a component on (1) server and if it dies we lose everything we can have a cluster and we can distribute the components (High Availabity)

We can also scale based on resource needs

Server		      Server
 Container (Pod)     	Container
 Container (Pod)	Container
 Container (Pod)        Container


Kube Master	
------------
Docker  // container runtime
Kubeadm // automates cluster setup
Kubelet // agent that manages processes
Kubectl // CLI tool to interact w/ the cluster
Control Plane // cluster controller

Kube Node (n)
--------------
Docker
Kubeadm
Kubelet
Kubectl

"Pods" are the smallest atomic unit (like cells) in the Kubernetes ecosystem. They consist of 
one or more containers, storage resources, and have a unique IP in the cluster network 

Node
  Pod (10.244.0.1)
     Container1

Node
  Pod (10.244.0.2)
     Container1
     Container2


Control Node		       
------------------
Kubernetes API
Other Core Components

Worker Node
-----------------
Pod 
Pod


KIND 3-Node Cluster Setup (DEV)
--------------------------------------------
kind create cluster --name lowk8s --wait 5m --config kind-multi-node.yaml (in ~/.kube)
kind delete cluster --name lowk8s

kubectl create deployment autodor --image=lwooden/testrun:firstry
kubectl get deployments
kubectl get pods
kubectl describe pod <POD NAME>
kubeclt describe service <deployment name>

kubectl expose deployment autodor  --type=Loadbalancer --port 8080 // does not work

To test connectivity to a application I have to:
 - find out what node the deployment was assigend to
 - ssh to that node
 - curl http://<NODE-IP>:<PORT>/ping
 
AWS EKS 1-Node Cluster Setup (DEV)
----------------------------------------------

eksctl create cluster --name=lowk8 --nodes=1 --node-type=m5.large --zones=us-east-1a,us-east-1b

eksctl delete cluster --region=us-east-1 --name=lowk8

kubectl port-forward pod/hello-kiamol 8080:80 // port forward to pod resource
kubectl port-forward deploy/hello-kiamol-2 8080:80 // port forward to deployment resource

kubectl apply -f pod.yml // send local file to the K8 API for deployment
kubectl apply -f https://github.com/pod.yaml  // send remote file to the K8 API for deployment

kubectl exec -it hello-kiamol-2-7f6dd54b9b-j6zvl -- sh // get interactive shell to your container

kubectl logs --tail=2 hello-kiamol-2-7f6dd54b9b-j6zvl // get logs from your container

kubectl cp hello-kiamol-2-7f6dd54b9b-j6zvl:/tmp/low.txt /Users/low/low.txt // move a file from pod to your local computer

kubectl get svc // get list of services

kubectl exec deploy/sleep -- printenv // get container environment variables

kubectl create configmap sleep-config-literal --from-literal=kiamol.section='4.1' // create a config map in your cluster

kubectl get cm sleep-config-literal // check config map

kubectl describe cm sleep-config-literal // friendly format


Resource Hierarchy
-------------------------

Services
     - LoadBalancer: integrates w/ an external loadbalancer which sends traffic to the cluster; traffic is associate with the back end pod based on port and label selector
     - ClusterIP: cluster wide ip that any pod can access; only works WITHIN the cluster; best used for comms between pods (internal)
     - NodePort: every node in the cluster listens on the port specified and sends traffic to the target port on the pod; can route external traffic into pods as well (like a loadbalancer)
     - ExternalName: creates a local alias in kube-dns that pods can use in to resolve external endpoints; the local alias will be used in the application as opposed to the true external domain name
  Deployments
            Pods

Commands
--------------------

kubeadm version 
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

kubectl version // check Kubernetes version ($Major: and $Minor:)
kubectl get nodes
kubectl get pods // displays default namespace
kubectl get pods -A // displays all namespaces
kubectl get pods -n kube-system // kube-system namespace
kubectl get pods -o wide // more detail
kubectl get deployments // displays list of deployments and their state
kubectl get service // displays list of services
kubectl get all

kubectl describe node $node_name
kubectl describe pods $pod_name
kubectl describe deployment $deployment_name

kubectl delete node $node_name
kubectl delete pod $pod_name
kubectl delete deployment $deployment_name

kubectl create namespace $name


Base Components
--------------------

[Kubernetes Control Plane]
etcd - provides distributed, synchronized data storage for the cluster state
kube-apiserver - serves the Kubernetes API, interface for the cluster
kube-controller-manager - bundles several components into one package
kube-scheduler - schedules pods to run on nodes; 

kubelet - agent that runs on each node and interfaces with the Kubernetes API and docker; runs as a systemd service (systemctl status kubelet
kube-proxy - handles the network layer inter/intra node traffic; runs on each node


Deployments
-------------------

"Deployments" are "objects" that gives us a way to organize and maintain our pods
- Scaling: specigy the number of replicas we want and that number of pods will be created
- Rolling Updates: gradually replace existing containers with a new update without taking them all out (no down-time)
- Self-Healing: if a pod happens to fail or get destroy, the deployment will immediately spin up another one to replace it in order to achieve the desired state (replica set)


# Deployment File Example

cat <<EOF | kubectl create -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx // tag name in Kubernetes
spec:
  replicas: 2 // desired state (number of Pods to run at all times)
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx    
        image: nginx:1.15.4   // container to pull from Docker Repo
        ports:
        - containerPort: 80   // port to expose
EOF


Services
---------------

"Services" provide an abstraction layer between upstream services and the underlying pods in order to get some information. 
The upstream service will send a request to the "service layer" and the service layer will distribute the request to the backend pods. 
This way no matter what is happening at the Pod Level, upstream services will be able to get their requests processed

Pod 1 <- [Service Layer] -> Resource

# Service File Example

cat << EOF | kubectl create -f -
kind: Service
apiVersion: v1
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx // determines which pods to route traffic to
  ports:
  - protocol: TCP
    port: 80  // port to route traffic to
    targetPort: 80
    nodePort: 30080 // listening port of the service layer
  type: NodePort
EOF


Install
--------
1. Install docker - All Nodes

// Get GPG key
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 

// Add Repo
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"

sudo apt-get update

sudo apt-get install -y docker-ce=18.06.1~ce~3-0~ubuntu

// Prevent package from being updated
sudo apt-mark hold docker-ce

// Verify
sudo docker version

2. Install Kubernetes Components - All Nodes

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

sudo apt-get update

sudo apt-get install -y kubelet=1.12.7-00 kubeadm=1.12.7-00 kubectl=1.12.7-00

sudo apt-mark hold kubelet kubeadm kubectl

3. Verify - All Nodes

kubeadm version 

4. Bootstrap Cluster - Master

sudo kubeadm init --pod-network-cidr=10.244.0.0/16

//Setup Local Kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

//Verify
kubectl version # check client and server response

5. Join Worker Nodes to Cluster

sudo kubeadm join $some_ip:6443 --token $some_token --discovery-token-ca-cert-hash $some_hash

6. Verify - Master

kubectl get nodes (won't be in READY STATE yet; networking not setup yet)

7. Set up Networking - Master (Flannel)

//Execute on all nodes
echo "net.bridge.bridge-nf-call-iptables=1" | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

//Grab config file from internet to apply to cluster
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml

//Verify Nodes are in READY State
kubectl get nodes

//Verify Back-End Services in Kube-System Namespace
kubectl get pods -n kube-system


kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml
